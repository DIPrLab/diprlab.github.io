<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Database Reading Group | DIPr Lab at PSU</title>
    <link>/dbrg/</link>
      <atom:link href="/dbrg/index.xml" rel="self" type="application/rss+xml" />
    <description>Database Reading Group</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 25 Apr 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/logo_hu_f67add51057eb433.png</url>
      <title>Database Reading Group</title>
      <link>/dbrg/</link>
    </image>
    
    <item>
      <title>Spring 2025 Week 4</title>
      <link>/dbrg/events/2025_spring_04/</link>
      <pubDate>Fri, 25 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/dbrg/events/2025_spring_04/</guid>
      <description>&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      Title
    &lt;/td&gt;
    &lt;td&gt;
      How good are query optimizers, really?
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
      Authors
    &lt;/td&gt;
    &lt;td&gt;
      Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, Thomas Neumann
    &lt;/td&gt;
  &lt;/tr&gt;
    &lt;tr&gt;
   &lt;td&gt;
      Abstract
    &lt;/td&gt;
    &lt;td&gt;
      Finding a good join order is crucial for query performance. In this paper, we introduce the Join Order Benchmark (JOB) and experimentally revisit the main components in the classic query optimizer architecture using a complex, real-world data set and realistic multi-join queries. We investigate the quality of industrial-strength cardinality estimators and find that all estimators routinely produce large errors. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using another set of experiments that measure the impact of the cost model, we find that it has much less influence on query performance than the cardinality estimates. Finally, we investigate plan enumeration techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the sub-optimal cardinality estimates.
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Spring 2025 Week 3</title>
      <link>/dbrg/events/2025_spring_03/</link>
      <pubDate>Fri, 18 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/dbrg/events/2025_spring_03/</guid>
      <description>&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      Title
    &lt;/td&gt;
    &lt;td&gt;
      PDX: A Data Layout for Vector Similarity Search
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
      Authors
    &lt;/td&gt;
    &lt;td&gt;
      Leonardo Kuffo, Elena Krippner, and Peter Boncz from CWI Amsterdam, The Netherlands
    &lt;/td&gt;
  &lt;/tr&gt;
    &lt;tr&gt;
   &lt;td&gt;
      Abstract
    &lt;/td&gt;
    &lt;td&gt;
      We propose Partition Dimensions Across (PDX), a data layout for vectors (e.g., embeddings) that, similar to PAX, stores multiple vectors in one block, using a vertical layout for the dimensions (Figure 1). PDX accelerates exact and approximate similarity search thanks to its dimension-by-dimension search strategy that operates on multiple-vectors-at-a-time in tight loops. It beats SIMD-optimized distance kernels on standard horizontal vector storage (avg 40% faster), only relying on scalar code that gets auto-vectorized. We combined the PDX layout with recent dimension-pruning algorithms ADSampling and BSA that accelerate approximate vector search. We found that these algorithms on the horizontal vector layout can lose to SIMD-optimized linear scans, even if they are SIMD-optimized. However, when used on PDX, their benefit is restored to 2-7x. We find that search on PDX is especially fast if a limited number of dimensions has to be scanned fully, which is what the dimension-pruning approaches do. We finally introduce PDX-BOND, an even more flexible dimension-pruning strategy, with good performance on exact search and reasonable performance on approximate search. Unlike previous pruning algorithms, it can work on vector data &#34;as-is&#34; without preprocessing; making it attractive for vector databases with frequent updates.
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Spring 2025 Week 1</title>
      <link>/dbrg/events/2025_spring_01/</link>
      <pubDate>Fri, 04 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/dbrg/events/2025_spring_01/</guid>
      <description>&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      Title
    &lt;/td&gt;
    &lt;td&gt;
      Navigating Labels and Vectors: A Unified Approach to Filtered Approximate Nearest Neighbor Search
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
      Authors
    &lt;/td&gt;
    &lt;td&gt;
      Yuzheng Cai, Jiayang Shi, Yizhuo Chen, Weigue Zheng
    &lt;/td&gt;
  &lt;/tr&gt;
    &lt;tr&gt;
   &lt;td&gt;
      Abstract
    &lt;/td&gt;
    &lt;td&gt;
      Given a query vector, approximate nearest neighbor search (ANNS) aims to retrieve similar vectors from a set of high-dimensional base vectors. However, many real-world applications jointly query both vector data and structured data, imposing label constraints such as attributes and keywords on the search, known as filtered ANNS. Effectively incorporating filtering conditions with vector similarity presents significant challenges, including index for dynamically filtered search space, agnostic query labels, computational overhead for label-irrelevant vectors, and potential inadequacy in returning results. To tackle these challenges, we introduce a novel approach called the Label Navigating Graph, which encodes the containment relationships of label sets for all vectors. Built upon graph-based ANNS methods, we develop a general framework termed Unified Navigating Graph (UNG) to bridge the gap between label set containment and vector proximity relations. UNG offers several advantages, including versatility in supporting any query label size and specificity, fidelity in exclusively searching filtered vectors, completeness in providing sufficient answers, and adaptability in integration with most graph-based ANNS algorithms. Extensive experiments on real datasets demonstrate that the proposed framework outperforms all baselines, achieving 10x speedups at the same accuracy.
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
</description>
    </item>
    
  </channel>
</rss>

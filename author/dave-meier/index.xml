<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dave Meier | DIPr Lab at PSU</title>
    <link>https://diprlab.github.io/author/dave-meier/</link>
      <atom:link href="https://diprlab.github.io/author/dave-meier/index.xml" rel="self" type="application/rss+xml" />
    <description>Dave Meier</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 12 Nov 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://diprlab.github.io/author/dave-meier/avatar_hu_386d2ffc11da7ada.png</url>
      <title>Dave Meier</title>
      <link>https://diprlab.github.io/author/dave-meier/</link>
    </image>
    
    <item>
      <title>Fall 2025 Week 7</title>
      <link>https://diprlab.github.io/dbrg/events/2025/fall/07/</link>
      <pubDate>Wed, 12 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://diprlab.github.io/dbrg/events/2025/fall/07/</guid>
      <description>&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      Title
    &lt;/td&gt;
    &lt;td&gt;
      Scribe: How Meta transports terabytes per second in real time
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
      Authors
    &lt;/td&gt;
    &lt;td&gt;
      Manos Karpathiotakis, et al.
    &lt;/td&gt;
  &lt;/tr&gt;
    &lt;tr&gt;
   &lt;td&gt;
      Abstract
    &lt;/td&gt;
    &lt;td&gt;
      Millions of web servers and a multitude of applications are producing ever-increasing amounts of data in real time at Meta. Regardless of how data is generated and how it is processed, there is a need for infrastructure that can accommodate the transport of arbitrarily large data streams from their generation location to their processing location with low latency. &lt;br /&gt; &lt;br /&gt;
      This paper presents Scribe, a multi-tenant message queue service that natively supports the requirements of Meta’s data-intensive applications, ingesting &gt; 15 TB/s and serving &gt; 110 TB/s to its consumers. Scribe relies on a multi-hop write path and opportunistic data placement to maximise write availability, whereas its read path adapts replica placement and representation based on the incoming workload as a means to minimise resource consumption for both Scribe and its downstreams. The wide range of Scribe use cases can pick from a range of offered guarantees, based on the trade-offs favourable for each one.
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Summer 2025 Week 1</title>
      <link>https://diprlab.github.io/dbrg/events/2025/summer/01/</link>
      <pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://diprlab.github.io/dbrg/events/2025/summer/01/</guid>
      <description>&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      Title
    &lt;/td&gt;
    &lt;td&gt;
      Streaming Democratized: Ease Across the Latency Spectrum with Delayed View Semantics and Snowflake Dynamic Tables
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
      Authors
    &lt;/td&gt;
    &lt;td&gt;
      Daniel Sotolongo, Daniel Mills, Tyler Akidau, Anirudh Santhiar, Attila-Péter Tóth, Botong Huang, Boyuan Zhang, Igor Belianski, Ling Geng, Matt Uhlar, Nikhil Shah, Olivia Zhou, Saras Nowak, Sasha Lionheart, Vlad Lifliand, Wendy Grus, Yiwen Zhu, Ankur Sharma, Dzmitry Pauliukevich, Enrico Sartorello, Ilaria Battiston, Ivan Kalev, Lawrence Benson, Leon Papke, Niklas Semmler, Till Merker, Yi Huang
    &lt;/td&gt;
  &lt;/tr&gt;
    &lt;tr&gt;
   &lt;td&gt;
      Abstract
    &lt;/td&gt;
    &lt;td&gt;
      Streaming data pipelines remain challenging and expensive to build and maintain, despite significant advancements in stronger consistency, event time semantics, and SQL support over the last decade. Persistent obstacles continue to hinder usability, such as the need for manual incrementalization, semantic discrepancies across SQL implementations, and the lack of enterprise-grade operational features (e.g. granular access control, disaster recovery). While the rise of incremental view maintenance (IVM) as a way to integrate streaming with databases has been a huge step forward, transaction isolation in the presence of IVM remains underspecified, which leaves the maintenance of application-level invariants as a painful exercise for the user. Meanwhile, most streaming systems optimize for latencies of 100 milliseconds to 3 seconds, whereas many practical use cases are well-served by latencies ranging from seconds to tens of minutes.
&lt;p&gt;In this paper, we present delayed view semantics (DVS), a conceptual foundation that bridges the semantic gap between streaming and databases, and introduce Dynamic Tables, Snowflake&amp;rsquo;s declarative streaming transformation primitive designed to democratize analytical stream processing. DVS formalizes the intuition that stream processing is primarily a technique to eagerly compute derived results asynchronously, while also addressing the need to reason about the resulting system end to end. Dynamic Tables then offer two key advantages: ease of use through DVS, enterprise-grade features, and simplicity; as well as scalable cost efficiency via IVM with an architecture designed for diverse latency requirements. We first develop extensions to transaction isolation that permit the preservation of invariants in streaming applications. We then detail the implementation challenges of Dynamic Tables and our experience operating it at scale. Finally, we share insights into user adoption and discuss our vision for the future of stream processing.&lt;/p&gt;
&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Spring 2025 Week 4</title>
      <link>https://diprlab.github.io/dbrg/events/2025/spring/04/</link>
      <pubDate>Fri, 25 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://diprlab.github.io/dbrg/events/2025/spring/04/</guid>
      <description>&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      Title
    &lt;/td&gt;
    &lt;td&gt;
      How good are query optimizers, really?
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
      Authors
    &lt;/td&gt;
    &lt;td&gt;
      Viktor Leis, Andrey Gubichev, Atanas Mirchev, Peter Boncz, Alfons Kemper, Thomas Neumann
    &lt;/td&gt;
  &lt;/tr&gt;
    &lt;tr&gt;
   &lt;td&gt;
      Abstract
    &lt;/td&gt;
    &lt;td&gt;
      Finding a good join order is crucial for query performance. In this paper, we introduce the Join Order Benchmark (JOB) and experimentally revisit the main components in the classic query optimizer architecture using a complex, real-world data set and realistic multi-join queries. We investigate the quality of industrial-strength cardinality estimators and find that all estimators routinely produce large errors. We further show that while estimates are essential for finding a good join order, query performance is unsatisfactory if the query engine relies too heavily on these estimates. Using another set of experiments that measure the impact of the cost model, we find that it has much less influence on query performance than the cardinality estimates. Finally, we investigate plan enumeration techniques comparing exhaustive dynamic programming with heuristic algorithms and find that exhaustive enumeration improves performance despite the sub-optimal cardinality estimates.
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
</description>
    </item>
    
  </channel>
</rss>
